{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel-h-warwick/warwick-aai-files/blob/main/CUDA_%26_GPU_Programming_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aca29f3",
      "metadata": {
        "id": "2aca29f3"
      },
      "source": [
        "# üöÄ Module Week 4: Accelerating Python Data Mining with GPUs\n",
        "\n",
        "## üìù STUDENT VERSION - Exercises with Scaffolding\n",
        "\n",
        "---\n",
        "\n",
        "## Workshop Overview\n",
        "\n",
        "**Course:** Advanced Data Mining & Analytics  \n",
        "**Module:** Week 4 - GPU-Accelerated Computing  \n",
        "**Environment:** Google Colab (T4 GPU - Free Tier)  \n",
        "**Duration:** ~2 hours hands-on\n",
        "\n",
        "**üìå NOTE:** This is the **STUDENT VERSION** with exercise scaffolding.\n",
        "Look for cells marked with **üèãÔ∏è EXERCISE** to complete the hands-on activities.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Learning Objectives\n",
        "\n",
        "By the end of this workshop, you will be able to:\n",
        "\n",
        "1. **Explain** the fundamental differences between CPU and GPU architectures\n",
        "2. **Benchmark** and quantify GPU speedups on real data operations\n",
        "3. **Write** basic CUDA kernels using Numba, understanding threads, blocks, and grids\n",
        "4. **Apply** RAPIDS cuDF for production-grade GPU-accelerated data mining\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è CRITICAL: Before You Begin\n",
        "\n",
        "**YOU MUST ENABLE GPU RUNTIME BEFORE RUNNING ANY CODE!**\n",
        "\n",
        "1. Go to **Runtime** ‚Üí **Change runtime type**\n",
        "2. Select **T4 GPU** under \"Hardware accelerator\"\n",
        "3. Click **Save**\n",
        "\n",
        "**If you skip this step, ALL GPU code will fail with cryptic errors!**\n",
        "\n",
        "---\n",
        "\n",
        "### üìã Prerequisites\n",
        "\n",
        "- Basic Python proficiency (NumPy, Pandas)\n",
        "- Understanding of fundamental data structures\n",
        "- Familiarity with Jupyter/Colab environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a5b80ac",
      "metadata": {
        "id": "4a5b80ac"
      },
      "source": [
        "---\n",
        "\n",
        "## üîß Section 0: Environment Setup & Validation\n",
        "\n",
        "This section ensures your environment is correctly configured. **Run each cell sequentially and verify the outputs.**\n",
        "\n",
        "### 0.1 GPU Hardware Verification\n",
        "\n",
        "**Expected Output:** You should see information about an NVIDIA T4 GPU with ~15GB memory.  \n",
        "**If you see \"No devices found\" or an error:** You forgot to enable GPU runtime!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4b8541a",
      "metadata": {
        "id": "a4b8541a"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 0.1: GPU HARDWARE VERIFICATION\n",
        "# =============================================================================\n",
        "# This cell MUST run successfully before proceeding.\n",
        "# If this fails, you have NOT enabled GPU runtime!\n",
        "# =============================================================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GPU HARDWARE VERIFICATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=True) # run nvidia-smi command or using the ! prefix in Colab\n",
        "    print(result.stdout)\n",
        "\n",
        "    # Parse GPU name for validation\n",
        "    if 'T4' in result.stdout:\n",
        "        print(\"‚úÖ SUCCESS: NVIDIA T4 GPU detected! You are ready to proceed.\")\n",
        "    elif 'Tesla' in result.stdout or 'A100' in result.stdout or 'V100' in result.stdout:\n",
        "        print(\"‚úÖ SUCCESS: NVIDIA GPU detected (different model). Proceeding...\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  WARNING: GPU detected but not T4. Results may vary.\")\n",
        "\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"‚ùå CRITICAL ERROR: nvidia-smi failed!\")\n",
        "    print(\"   This means NO GPU is available.\")\n",
        "    print(\"\")\n",
        "    print(\"   FIX: Go to Runtime ‚Üí Change runtime type ‚Üí Select 'T4 GPU' ‚Üí Save\")\n",
        "    print(\"   Then restart the runtime and run this cell again.\")\n",
        "    sys.exit(1)\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå CRITICAL ERROR: nvidia-smi not found!\")\n",
        "    print(\"   You are likely not running on Google Colab or GPU is not enabled.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf501d9",
      "metadata": {
        "id": "3cf501d9"
      },
      "source": [
        "### 0.2 Setup Numba & Visualization\n",
        "\n",
        "**Run this cell to import basic libraries.**\n",
        "We will use `numba` for writing CUDA kernels and `matplotlib` for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "286eac46",
      "metadata": {
        "id": "286eac46"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 0.2: IMPORT NUMBA & BASICS\n",
        "# =============================================================================\n",
        "# Import libraries needed for Part 1 (CUDA Kernels).\n",
        "# =============================================================================\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import time\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# Colors\n",
        "COLORS = {\n",
        "    'cpu': '#3498db', 'gpu': '#e74c3c', 'gpu_transfer': '#f39c12',\n",
        "    'gpu_compute': '#27ae60', 'neutral': '#95a5a6', 'highlight': '#9b59b6',\n",
        "    'success': '#2ecc71', 'warning': '#f1c40f'\n",
        "}\n",
        "\n",
        "try:\n",
        "    from numba import cuda\n",
        "    import numba\n",
        "    NUMBA_AVAILABLE = True\n",
        "    print(f\"‚úÖ Numba version: {numba.__version__}\")\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    print(\"‚ùå Numba not found\")\n",
        "\n",
        "if NUMBA_AVAILABLE:\n",
        "    try:\n",
        "        cuda.detect()\n",
        "        print(f\"‚úÖ CUDA device: {cuda.get_current_device().name.decode('utf-8')}\")\n",
        "    except:\n",
        "        print(\"‚ùå No CUDA device detected\")\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# =============================================================================\n",
        "# üõ†Ô∏è VISUALIZATION UTILITIES\n",
        "# =============================================================================\n",
        "\n",
        "def visualize_cuda_hierarchy(data_size=20, threads_per_block=4):\n",
        "    \"\"\"Visualizes Grid/Block/Thread hierarchy.\"\"\"\n",
        "    num_blocks = (data_size + threads_per_block - 1) // threads_per_block\n",
        "    total_threads = num_blocks * threads_per_block\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))\n",
        "\n",
        "    # Data Array\n",
        "    for i in range(total_threads):\n",
        "        color = COLORS['cpu'] if i < data_size else COLORS['neutral']\n",
        "        alpha = 1.0 if i < data_size else 0.3\n",
        "        rect = plt.Rectangle((i, 0), 0.9, 1, facecolor=color, alpha=alpha, edgecolor='black')\n",
        "        ax1.add_patch(rect)\n",
        "        label = str(i) if i < data_size else '‚àÖ'\n",
        "        ax1.text(i+0.45, 0.5, label, ha='center', va='center')\n",
        "\n",
        "    ax1.set_xlim(-0.5, total_threads+0.5)\n",
        "    ax1.set_ylim(0, 1.5)\n",
        "    ax1.axis('off')\n",
        "    ax1.set_title(f'Data Array ({data_size} elements)', fontweight='bold')\n",
        "\n",
        "    # Blocks\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, num_blocks))\n",
        "    for b in range(num_blocks):\n",
        "        start = b * threads_per_block\n",
        "        # Block outline\n",
        "        rect = plt.Rectangle((start-0.1, 0), threads_per_block+0.1, 1.5, fill=False, edgecolor=colors[b], linewidth=3)\n",
        "        ax2.add_patch(rect)\n",
        "        ax2.text(start + threads_per_block/2, 1.6, f'Block {b}', ha='center', color=colors[b], fontweight='bold')\n",
        "\n",
        "        # Threads\n",
        "        for t in range(threads_per_block):\n",
        "            g_idx = start + t\n",
        "            color = colors[b] if g_idx < data_size else COLORS['neutral']\n",
        "            rect = plt.Rectangle((g_idx, 0.2), 0.8, 1, facecolor=color, alpha=0.7, edgecolor='black')\n",
        "            ax2.add_patch(rect)\n",
        "            ax2.text(g_idx+0.4, 0.7, f'T{t}', ha='center', fontsize=8)\n",
        "            ax2.text(g_idx+0.4, 0.4, f'[{g_idx}]', ha='center', fontsize=7)\n",
        "\n",
        "    ax2.set_xlim(-0.5, total_threads+0.5)\n",
        "    ax2.set_ylim(0, 2)\n",
        "    ax2.axis('off')\n",
        "    ax2.set_title(f'Grid: {num_blocks} Blocks √ó {threads_per_block} Threads', fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_timing_breakdown_viz(transfer_to, kernel, transfer_from):\n",
        "    \"\"\"Visualizes GPU timing breakdown.\"\"\"\n",
        "    total = transfer_to + kernel + transfer_from\n",
        "    times = [transfer_to*1000, kernel*1000, transfer_from*1000]\n",
        "    labels = ['Upload', 'Compute', 'Download']\n",
        "    colors = [COLORS['gpu_transfer'], COLORS['gpu_compute'], COLORS['highlight']]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 2))\n",
        "    left = 0\n",
        "    for t, l, c in zip(times, labels, colors):\n",
        "        ax.barh(0, t, left=left, color=c, label=l, height=0.5)\n",
        "        if t > total*1000*0.05: # Only label if wide enough\n",
        "            ax.text(left + t/2, 0, f'{t:.2f}ms', ha='center', va='center', color='white', fontweight='bold')\n",
        "        left += t\n",
        "\n",
        "    ax.set_title('GPU Execution Timeline')\n",
        "    ax.set_xlabel('Time (ms)')\n",
        "    ax.set_yticks([])\n",
        "    ax.legend(loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Numba & Visualization utilities loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd1a5209",
      "metadata": {
        "id": "dd1a5209"
      },
      "source": [
        "---\n",
        "\n",
        "## üî¨ Part 1: Fundamentals - The CUDA Programming Model\n",
        "\n",
        "### 1.1 The CUDA Hierarchy: Grid ‚Üí Block ‚Üí Thread\n",
        "\n",
        "**The Concept:**\n",
        "Imagine a factory.\n",
        "*   **Thread:** One worker.\n",
        "*   **Block:** A team of workers (who can talk to each other).\n",
        "*   **Grid:** The entire factory workforce.\n",
        "\n",
        "**The Architecture:**\n",
        "*   **Host (CPU):** The Manager. Sends work to the GPU.\n",
        "*   **Device (GPU):** The Factory. Has thousands of cores (workers).\n",
        "*   **PCI-E Bus:** The Bridge. Moving data across this is SLOW.\n",
        "\n",
        "**Key Rule:**\n",
        "*   **Too few threads?** Workers sit idle.\n",
        "*   **Too many threads?** Factory gets crowded.\n",
        "*   **Goal:** Keep all cores busy!\n",
        "\n",
        "**Why this matters:** You must tell CUDA how to divide your work!\n",
        "- Too few threads ‚Üí GPU cores sit idle\n",
        "- Too many threads ‚Üí Overhead and memory issues\n",
        "- Wrong block size ‚Üí Performance degradation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c294037",
      "metadata": {
        "id": "1c294037"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 1.1: VISUALIZE THE THREAD HIERARCHY\n",
        "# =============================================================================\n",
        "# Run this to see how Grid, Blocks, and Threads are organized.\n",
        "# =============================================================================\n",
        "\n",
        "visualize_cuda_hierarchy(data_size=20, threads_per_block=8)\n",
        "\n",
        "print(\"\\nüí° KEY CONCEPT: GLOBAL INDEX CALCULATION\")\n",
        "print(\"   idx = blockIdx.x * blockDim.x + threadIdx.x\")\n",
        "print(\"   This formula gives every thread a unique ID across the whole grid.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cc82164",
      "metadata": {
        "id": "5cc82164"
      },
      "source": [
        "### 1.2 Your First CUDA Kernel: 1D Grids\n",
        "\n",
        "**The \"Hello World\" of GPU Programming**\n",
        "\n",
        "We will use a **\"Show One, Do One\"** approach.\n",
        "1. We **SHOW** you a working kernel (Array Doubling).\n",
        "2. You **DO** a similar kernel (Vector Addition).\n",
        "\n",
        "**Why this is perfect for GPUs:** Each element can be computed independently!\n",
        "- Element 0: Thread 0 computes C[0]\n",
        "- Element 1: Thread 1 computes C[1]\n",
        "- Element N: Thread N computes C[N]\n",
        "\n",
        "**All threads execute simultaneously!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc5eac3f",
      "metadata": {
        "id": "cc5eac3f"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DEMO 1.2: ARRAY DOUBLING KERNEL\n",
        "# =============================================================================\n",
        "# A complete example of a 1D kernel.\n",
        "# =============================================================================\n",
        "\n",
        "from numba import cuda\n",
        "import numpy as np\n",
        "\n",
        "# 1. THE KERNEL (Runs on GPU)\n",
        "@cuda.jit\n",
        "def double_kernel(array):\n",
        "    # Calculate unique thread ID\n",
        "    # idx = Block Index * Block Size + Thread Index\n",
        "    idx = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "\n",
        "    # Bounds check: Ensure we don't go past the end of the array\n",
        "    if idx < array.size:\n",
        "        array[idx] *= 2  # Multiply by 2 in-place\n",
        "\n",
        "# 2. THE LAUNCH (Runs on CPU)\n",
        "print(\"üöÄ Running Demo: Array Doubling...\")\n",
        "\n",
        "# Create data\n",
        "data = np.arange(10).astype(np.float32)\n",
        "d_data = cuda.to_device(data) # Move to GPU\n",
        "\n",
        "# Configure Grid\n",
        "threads_per_block = 256\n",
        "# Calculate blocks needed: (N + threads - 1) // threads\n",
        "blocks = (d_data.size + threads_per_block - 1) // threads_per_block\n",
        "\n",
        "# Launch\n",
        "double_kernel[blocks, threads_per_block](d_data)\n",
        "cuda.synchronize()\n",
        "\n",
        "# Check result\n",
        "print(f\"   Input:  {np.arange(10)}\")\n",
        "print(f\"   Output: {d_data.copy_to_host()}\")\n",
        "print(\"‚úÖ Demo Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0fc534",
      "metadata": {
        "id": "bc0fc534"
      },
      "source": [
        "### 1.2 Demo: Your First Kernel (Array Doubling)\n",
        "\n",
        "**Concept:** 1D Grid.\n",
        "**Task:** Multiply every element in an array by 2.\n",
        "\n",
        "We will **SHOW** you this one. Read the code carefully!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2395384a",
      "metadata": {
        "id": "2395384a"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üèãÔ∏è EXERCISE 1.3: VECTOR ADDITION KERNEL\n",
        "# =============================================================================\n",
        "# YOUR TURN!\n",
        "# Task: Modify the pattern from the Demo to add two arrays: c[i] = a[i] + b[i]\n",
        "# =============================================================================\n",
        "\n",
        "# 1. DEFINE THE KERNEL\n",
        "@cuda.jit\n",
        "def vector_add_kernel(a, b, c):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO 1: Calculate global thread index (Look at the Demo!)\n",
        "    # -------------------------------------------------------------------------\n",
        "    idx = 0 # üî¥ YOUR CODE HERE (Hint: cuda.blockIdx.x * ... + ...)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO 2: Bounds check & Addition\n",
        "    # -------------------------------------------------------------------------\n",
        "    if False: # üî¥ YOUR CODE HERE (Check if idx < a.size)\n",
        "        pass # üî¥ YOUR CODE HERE (c[idx] = a[idx] + b[idx])\n",
        "\n",
        "# 2. LAUNCH THE KERNEL\n",
        "print(\"üöÄ Launching Vector Add Kernel...\")\n",
        "N = 1_000_000\n",
        "a = cuda.to_device(np.ones(N))\n",
        "b = cuda.to_device(np.ones(N))\n",
        "c = cuda.device_array(N)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# TODO 3: Configure Grid\n",
        "# -----------------------------------------------------------------------------\n",
        "threads_per_block = 256\n",
        "blocks_per_grid = 1 # üî¥ YOUR CODE HERE (Calculate blocks needed for N)\n",
        "\n",
        "print(f\"   Threads: {threads_per_block}, Blocks: {blocks_per_grid}\")\n",
        "\n",
        "# Launch!\n",
        "vector_add_kernel[blocks_per_grid, threads_per_block](a, b, c)\n",
        "cuda.synchronize()\n",
        "\n",
        "# Verify\n",
        "res = c.copy_to_host()\n",
        "if np.allclose(res, 2.0):\n",
        "    print(\"‚úÖ SUCCESS! Kernel worked.\")\n",
        "else:\n",
        "    print(\"‚ùå FAIL. Check your kernel logic.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9532712",
      "metadata": {
        "id": "e9532712"
      },
      "source": [
        "### üå∂Ô∏è SPICY CHALLENGE (Optional)\n",
        "**Finished early?**\n",
        "Modify the kernel to compute `c[i] = a[i] * b[i] + a[i]`.\n",
        "*Hint: This is the famous SAXPY pattern (Single-Precision A*X Plus Y).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42944167",
      "metadata": {
        "id": "42944167"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 1.3: WHERE DOES THE TIME GO?\n",
        "# =============================================================================\n",
        "# We use a utility to visualize transfer vs compute time.\n",
        "# =============================================================================\n",
        "\n",
        "# Typical values for this operation\n",
        "create_timing_breakdown_viz(transfer_to=0.002, kernel=0.0001, transfer_from=0.001)\n",
        "\n",
        "print(\"üí° INSIGHT: The kernel is instant. Moving data takes all the time!\")\n",
        "print(\"   Rule: Keep data on the GPU!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7296ca07",
      "metadata": {
        "id": "7296ca07"
      },
      "source": [
        "### 1.4 Concept: 2D Grids\n",
        "\n",
        "**Why 2D?**\n",
        "- Images (Height √ó Width)\n",
        "- Matrices (Rows √ó Cols)\n",
        "- Neural Networks (Layers)\n",
        "\n",
        "**Parallelism:** Each output element (x, y) can be computed independently!\n",
        "\n",
        "We will use the same **\"Show One, Do One\"** approach:\n",
        "1. We **SHOW** you a visual 2D kernel (Gradient).\n",
        "2. You **DO** a computational 2D kernel (Matrix Multiplication).\n",
        "\n",
        "**This kernel demonstrates:**\n",
        "1. 2D grid indexing (`cuda.grid(2)`)\n",
        "2. Bounds checking in 2D\n",
        "3. Performance comparison CPU vs GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "048cd44b",
      "metadata": {
        "id": "048cd44b"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DEMO 1.4: 2D GRADIENT KERNEL\n",
        "# =============================================================================\n",
        "# A complete example of a 2D kernel.\n",
        "# =============================================================================\n",
        "\n",
        "@cuda.jit\n",
        "def gradient_kernel(image):\n",
        "    # Get 2D coordinates\n",
        "    # cuda.grid(2) returns (x, y) corresponding to the grid dimensions\n",
        "    # We map x -> row (dim 0), y -> col (dim 1)\n",
        "    row, col = cuda.grid(2)\n",
        "\n",
        "    # Dimensions\n",
        "    height, width = image.shape\n",
        "\n",
        "    # Bounds check\n",
        "    if row < height and col < width:\n",
        "        # Set pixel value based on position\n",
        "        image[row, col] = (row + col) / (height + width)\n",
        "\n",
        "print(\"üöÄ Running Demo: 2D Gradient...\")\n",
        "\n",
        "# Create 1024x1024 image\n",
        "img_gpu = cuda.device_array((1024, 1024), dtype=np.float32)\n",
        "\n",
        "# Configure 2D Grid\n",
        "threads = (16, 16)\n",
        "# Calculate blocks needed for Height (Rows) and Width (Cols)\n",
        "blocks_rows = (1024 + 15) // 16\n",
        "blocks_cols = (1024 + 15) // 16\n",
        "\n",
        "# Launch: (Grid Dim 0, Grid Dim 1) -> (Rows, Cols)\n",
        "gradient_kernel[(blocks_rows, blocks_cols), threads](img_gpu)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(img_gpu.copy_to_host(), cmap='viridis')\n",
        "plt.title(\"Generated on GPU\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "964b2dd4",
      "metadata": {
        "id": "964b2dd4"
      },
      "source": [
        "### 1.4 Demo: 2D Kernel (Visual Pattern)\n",
        "\n",
        "**Concept:** 2D Grid.\n",
        "**Task:** Create a visual gradient based on (x, y) coordinates.\n",
        "\n",
        "We will **SHOW** you how to handle 2D coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ade532e",
      "metadata": {
        "id": "4ade532e"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üèãÔ∏è EXERCISE 1.5: MATRIX MULTIPLICATION KERNEL\n",
        "# =============================================================================\n",
        "# YOUR TURN!\n",
        "# Task: Implement a 2D kernel for C = A @ B\n",
        "# NOTE: This is a \"Naive\" implementation for learning. It will be slow!\n",
        "#       Production kernels use Shared Memory tiling (much more complex).\n",
        "# =============================================================================\n",
        "\n",
        "@cuda.jit\n",
        "def matmul_kernel(A, B, C):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO 1: Get 2D thread indices\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Hint: cuda.grid(2) returns (row, col) if grid is (blocks_rows, blocks_cols)\n",
        "    row, col = 0, 0 # üî¥ YOUR CODE HERE\n",
        "\n",
        "    # Dimensions\n",
        "    M, N = C.shape\n",
        "    K = A.shape[1]\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO 2: Bounds Check & Dot Product\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Check if row < M and col < N\n",
        "    if False: # üî¥ YOUR CODE HERE\n",
        "        temp = 0.0\n",
        "        # Compute dot product of Row A and Col B\n",
        "        for k in range(K):\n",
        "            temp += A[row, k] * B[k, col]\n",
        "        C[row, col] = temp\n",
        "\n",
        "# LAUNCH\n",
        "print(\"üöÄ Launching MatMul Kernel...\")\n",
        "M, K, N = 1024, 512, 1024\n",
        "A = cuda.to_device(np.random.rand(M, K))\n",
        "B = cuda.to_device(np.random.rand(K, N))\n",
        "C = cuda.device_array((M, N))\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# TODO 3: Configure 2D Grid\n",
        "# -----------------------------------------------------------------------------\n",
        "threads_2d = (16, 16)\n",
        "# Calculate blocks needed for M (Rows) and N (Cols)\n",
        "# Hint: (Total Size + Block Size - 1) // Block Size\n",
        "blocks_rows = 1 # üî¥ YOUR CODE HERE (Blocks for M)\n",
        "blocks_cols = 1 # üî¥ YOUR CODE HERE (Blocks for N)\n",
        "\n",
        "matmul_kernel[(blocks_rows, blocks_cols), threads_2d](A, B, C)\n",
        "cuda.synchronize()\n",
        "\n",
        "print(\"‚úÖ Kernel finished (Verify logic if needed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e9ca44a",
      "metadata": {
        "id": "6e9ca44a"
      },
      "source": [
        "### 1.5 Part 1 Summary: CUDA Fundamentals\n",
        "\n",
        "**What You Learned:**\n",
        "1. **Memory:** Host (CPU) vs Device (GPU). Moving data is slow!\n",
        "2. **Hierarchy:** Grid > Block > Thread.\n",
        "3. **Indexing:** `cuda.grid(1)` or `cuda.grid(2)` gives you the unique ID.\n",
        "4. **Configuration:** You must calculate how many blocks you need.\n",
        "\n",
        "**Why This Matters:**\n",
        "- **Debug:** Understand why `out of memory` happens.\n",
        "- **Optimize:** Know that data transfer is the enemy.\n",
        "- **Custom:** Write kernels when cuDF can't do it.\n",
        "\n",
        "\n",
        "**Why This Matters for Data Mining:**\n",
        "\n",
        "Understanding these fundamentals helps you:\n",
        "- Know when GPU acceleration will help (large, parallel operations)\n",
        "- Debug performance issues (is transfer time dominating?)\n",
        "- Write custom kernels when libraries don't support your operation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe37f6c",
      "metadata": {
        "id": "5fe37f6c"
      },
      "source": [
        "## üîß Section 2: Install RAPIDS cuDF Library\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT:** This installation takes 2-5 minutes. Be patient!\n",
        "\n",
        "**‚ö†Ô∏è RESTART REQUIRED:** After this cell completes successfully, you **MUST** restart the runtime:\n",
        "- Go to **Runtime** ‚Üí **Restart runtime** (or press Ctrl+M, then .)\n",
        "- Do NOT re-run the installation cell after restarting\n",
        "- Continue from Section 3\n",
        "\n",
        "**Why pip instead of conda?** Google Colab's environment is pre-configured. The pip installation method is now the officially recommended approach for Colab and avoids the complex conda environment issues that plagued earlier RAPIDS installations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66e46a91",
      "metadata": {
        "id": "66e46a91"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 2.1: INSTALL RAPIDS cuDF (GPU-ACCELERATED DATAFRAMES)\n",
        "# =============================================================================\n",
        "# This is the OFFICIAL pip installation method for RAPIDS on Colab.\n",
        "# DO NOT use conda - it will cause environment conflicts on Colab!\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"INSTALLING RAPIDS cuDF - THIS WILL TAKE 2-5 MINUTES\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\")\n",
        "print(\"‚òï Grab a coffee while this installs...\")\n",
        "print(\"\")\n",
        "\n",
        "# Install cuDF using the official NVIDIA PyPI index\n",
        "# The --extra-index-url points to NVIDIA's package repository\n",
        "# cu12 = CUDA 12.x compatibility (matches Colab's CUDA version)\n",
        "\n",
        "try:\n",
        "    import cudf\n",
        "    print(\"‚úÖ cuDF is already installed. Skipping installation.\")\n",
        "    print(\"   If you are just starting, ensure you have restarted the runtime if prompted previously.\")\n",
        "except ImportError:\n",
        "    print(\"   Installing cudf-cu12...\")\n",
        "    !pip install --quiet cudf-cu12 --extra-index-url=https://pypi.nvidia.com\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"‚úÖ INSTALLATION COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\")\n",
        "    print(\"‚ö†Ô∏è  CRITICAL NEXT STEP:\")\n",
        "    print(\"   You MUST restart the runtime NOW!\")\n",
        "    print(\"\")\n",
        "    print(\"   Go to: Runtime ‚Üí Restart runtime\")\n",
        "    print(\"   OR press: Ctrl+M, then . (period)\")\n",
        "    print(\"\")\n",
        "    print(\"   After restart, SKIP this cell and continue from Section 3\")\n",
        "    print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c0f466",
      "metadata": {
        "id": "57c0f466"
      },
      "source": [
        "### 3.1 Verify Installation & Import Libraries\n",
        "\n",
        "**Run this cell AFTER restarting the runtime.**\n",
        "\n",
        "If you see import errors for `cudf`, you either:\n",
        "1. Did not restart the runtime after installation, OR\n",
        "2. The installation failed (scroll up to check for errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49bd989d",
      "metadata": {
        "id": "49bd989d"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 3.1: IMPORT RAPIDS & WORKSHOP UTILITIES\n",
        "# =============================================================================\n",
        "# Run this cell to import libraries and define helper functions.\n",
        "# You do NOT need to read the code below - it handles visualization and setup.\n",
        "# =============================================================================\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import gc\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check for GPU libraries\n",
        "try:\n",
        "    import cudf\n",
        "    CUDF_AVAILABLE = True\n",
        "    print(f\"‚úÖ cuDF version: {cudf.__version__}\")\n",
        "except ImportError:\n",
        "    CUDF_AVAILABLE = False\n",
        "    print(\"‚ùå cuDF not found\")\n",
        "\n",
        "# =============================================================================\n",
        "# üõ†Ô∏è WORKSHOP UTILITIES (VISUALIZATION & DATA GEN)\n",
        "# =============================================================================\n",
        "\n",
        "def generate_synthetic_dataset(n_rows=10_000_000):\n",
        "    \"\"\"Generates the e-commerce dataset for the workshop.\"\"\"\n",
        "    print(f\"Generating {n_rows:,} rows of synthetic data...\")\n",
        "    start = time.time()\n",
        "\n",
        "    # Constants\n",
        "    n_customers = 100_000\n",
        "    n_products = 5_000\n",
        "    n_categories = 50\n",
        "\n",
        "    # Generate arrays\n",
        "    ids = np.arange(1, n_rows + 1, dtype=np.int64)\n",
        "    cust_ids = np.random.randint(1, n_customers + 1, size=n_rows, dtype=np.int32)\n",
        "    prod_ids = np.random.randint(1, n_products + 1, size=n_rows, dtype=np.int32)\n",
        "    cat_ids = (prod_ids % n_categories) + 1\n",
        "    qty = np.random.randint(1, 11, size=n_rows, dtype=np.int16)\n",
        "\n",
        "    # Prices (log-normal)\n",
        "    prices = np.round(np.random.lognormal(3.0, 1.0, size=n_rows), 2)\n",
        "    prices = np.clip(prices, 0.99, 999.99).astype(np.float32)\n",
        "\n",
        "    # Discounts\n",
        "    disc_opts = np.array([0.0, 0.05, 0.10, 0.15, 0.20, 0.25], dtype=np.float32)\n",
        "    discs = np.random.choice(disc_opts, size=n_rows)\n",
        "\n",
        "    regions = np.random.choice(['North', 'South', 'East', 'West', 'Central'], size=n_rows)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'transaction_id': ids, 'customer_id': cust_ids, 'product_id': prod_ids,\n",
        "        'category_id': cat_ids, 'quantity': qty, 'unit_price': prices,\n",
        "        'discount': discs, 'region': regions\n",
        "    })\n",
        "\n",
        "    df['total_amount'] = (df['quantity'] * df['unit_price'] * (1 - df['discount'])).astype(np.float32)\n",
        "\n",
        "    # Add dates\n",
        "    base_date = pd.Timestamp('2024-11-27')\n",
        "    days_ago = np.random.randint(0, 365, size=n_rows)\n",
        "    df['transaction_date'] = base_date - pd.to_timedelta(days_ago, unit='D')\n",
        "\n",
        "    print(f\"‚úÖ Dataset generated in {time.time()-start:.2f}s\")\n",
        "    print(f\"   Memory: {df.memory_usage(deep=True).sum()/1e6:.1f} MB\")\n",
        "    return df\n",
        "\n",
        "def create_benchmark_bar_chart(cpu_time, gpu_time, operation_name, cpu_times_list=None, gpu_times_list=None):\n",
        "    \"\"\"Visualizes CPU vs GPU performance.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Bar Chart\n",
        "    ax1 = axes[0]\n",
        "    times = [cpu_time, gpu_time if gpu_time else 0]\n",
        "    colors = [COLORS['cpu'], COLORS['gpu'] if gpu_time else COLORS['neutral']]\n",
        "    ax1.bar(['CPU\\n(Pandas)', 'GPU\\n(cuDF)'], times, color=colors, edgecolor='black')\n",
        "\n",
        "    for i, v in enumerate(times):\n",
        "        if v > 0: ax1.text(i, v, f'{v:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    ax1.set_title(f'{operation_name}: Execution Time')\n",
        "    ax1.set_ylabel('Seconds')\n",
        "\n",
        "    # Speedup Chart\n",
        "    ax2 = axes[1]\n",
        "    if gpu_time and gpu_time > 0:\n",
        "        speedup = cpu_time / gpu_time\n",
        "        ax2.barh(['Speedup'], [speedup], color=COLORS['gpu'], edgecolor='black', height=0.5)\n",
        "        ax2.axvline(1, color='black', linestyle='--')\n",
        "        ax2.text(speedup, 0, f' {speedup:.1f}x FASTER!', va='center', fontweight='bold', color=COLORS['gpu'], fontsize=14)\n",
        "        ax2.set_title('üöÄ GPU Speedup Factor')\n",
        "    else:\n",
        "        ax2.text(0.5, 0.5, 'GPU Benchmark Skipped', ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_combined_benchmark_summary(benchmarks):\n",
        "    \"\"\"Summary of all benchmarks.\"\"\"\n",
        "    ops = [b['name'] for b in benchmarks]\n",
        "    speedups = [b['cpu_time']/b['gpu_time'] if b['gpu_time'] else 0 for b in benchmarks]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bars = ax.barh(ops, speedups, color=COLORS['gpu'])\n",
        "    ax.axvline(1, color='black', linestyle='--')\n",
        "    ax.set_title('üèÜ Final Speedup Summary')\n",
        "    ax.set_xlabel('Speedup Factor (x)')\n",
        "\n",
        "    for bar, s in zip(bars, speedups):\n",
        "        ax.text(s, bar.get_y() + bar.get_height()/2, f' {s:.1f}x', va='center', fontweight='bold')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_rfm_segments(rfm_data):\n",
        "    \"\"\"Visualizes RFM segments.\"\"\"\n",
        "    if isinstance(rfm_data, cudf.DataFrame):\n",
        "        rfm_data = rfm_data.to_pandas()\n",
        "\n",
        "    # Simple segmentation logic for viz\n",
        "    def get_segment(r):\n",
        "        if r['RFM_score'] >= 12: return 'Champions'\n",
        "        if r['RFM_score'] <= 6: return 'Lost'\n",
        "        return 'Regular'\n",
        "\n",
        "    rfm_data['Segment'] = rfm_data.apply(get_segment, axis=1)\n",
        "    counts = rfm_data['Segment'].value_counts()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Pie\n",
        "    ax1.pie(counts, labels=counts.index, autopct='%1.1f%%', colors=[COLORS['success'], COLORS['warning'], COLORS['cpu']])\n",
        "    ax1.set_title('Customer Segments')\n",
        "\n",
        "    # Scatter\n",
        "    sample = rfm_data.sample(min(5000, len(rfm_data)))\n",
        "    ax2.scatter(sample['frequency'], sample['monetary'], c=sample['RFM_score'], cmap='RdYlGn', alpha=0.5)\n",
        "    ax2.set_xlabel('Frequency')\n",
        "    ax2.set_ylabel('Monetary')\n",
        "    ax2.set_yscale('log')\n",
        "    ax2.set_title('Frequency vs Monetary (Color = RFM Score)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Workshop utilities loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "528e125e",
      "metadata": {
        "id": "528e125e"
      },
      "source": [
        "---\n",
        "\n",
        "## üìä Part 2: The \"Why\" - Benchmarking CPU vs. GPU\n",
        "\n",
        "### Motivation\n",
        "\n",
        "Before diving into *how* GPUs work, let's see *why* we should care. We'll perform identical operations on CPU (Pandas) and GPU (cuDF) and measure the difference.\n",
        "\n",
        "**Key Insight:** GPUs excel at **throughput** - doing many simple operations simultaneously. CPUs excel at **latency** - doing one complex operation quickly.\n",
        "\n",
        "**Analogy:**\n",
        "- **CPU** = Ferrari üèéÔ∏è - Very fast for one passenger, but can only carry 1-2 people\n",
        "- **GPU** = Bus üöå - Slower per trip, but carries 50+ people simultaneously\n",
        "\n",
        "For data mining with millions of rows, we need the bus!\n",
        "\n",
        "### 2.1 Generate Synthetic Dataset\n",
        "\n",
        "We'll create a realistic e-commerce transaction dataset with 10 million rows. This ensures:\n",
        "1. **Reproducibility** - No external file dependencies\n",
        "2. **Scale** - Large enough to see meaningful speedups\n",
        "3. **Realism** - Multiple data types (int, float, string, datetime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d24bd491",
      "metadata": {
        "id": "d24bd491"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 2.1: GENERATE SYNTHETIC DATASET\n",
        "# =============================================================================\n",
        "# We use a helper function to generate 10 million rows of transaction data.\n",
        "# =============================================================================\n",
        "\n",
        "# Generate the dataset (this might take 10-20 seconds)\n",
        "df_pandas = generate_synthetic_dataset(n_rows=10_000_000)\n",
        "\n",
        "print(\"\\nSample Data:\")\n",
        "print(df_pandas.head())\n",
        "print(f\"\\nData Types:\\n{df_pandas.dtypes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4a56536",
      "metadata": {
        "id": "d4a56536"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DEMO 2.1: HELLO cuDF\n",
        "# =============================================================================\n",
        "# A simple demonstration of creating and using a GPU DataFrame.\n",
        "# =============================================================================\n",
        "\n",
        "if CUDF_AVAILABLE:\n",
        "    print(\"üü¢ CREATING GPU DATAFRAME...\")\n",
        "\n",
        "    # 1. Create directly on GPU\n",
        "    gdf_demo = cudf.DataFrame({\n",
        "        'a': [1, 2, 3, 4, 5],\n",
        "        'b': [10, 20, 30, 40, 50]\n",
        "    })\n",
        "\n",
        "    # 2. Perform operation (happens on GPU!)\n",
        "    gdf_demo['c'] = gdf_demo['a'] * gdf_demo['b']\n",
        "\n",
        "    print(\"   Result (looks like Pandas):\")\n",
        "    print(gdf_demo)\n",
        "\n",
        "    print(f\"\\n   Type: {type(gdf_demo)}\")\n",
        "\n",
        "    del gdf_demo\n",
        "else:\n",
        "    print(\"‚ùå cuDF not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db2ef297",
      "metadata": {
        "id": "db2ef297"
      },
      "source": [
        "### 2.1 Demo: Hello cuDF!\n",
        "\n",
        "**Watch this first.** We'll create a small DataFrame on the GPU and perform a simple operation.\n",
        "Notice how it looks exactly like Pandas!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "281f504a",
      "metadata": {
        "id": "281f504a"
      },
      "source": [
        "### 2.2 Benchmark: Sorting (CPU vs. GPU)\n",
        "\n",
        "**Operation:** Sort 10 million rows by `total_amount` column.\n",
        "\n",
        "**üõë STOP & PREDICT:**\n",
        "Before running the code, guess the speedup factor.\n",
        "- CPU Time: ~2-5 seconds (typically)\n",
        "- GPU Time: ???\n",
        "- **My Prediction:** ________ x faster\n",
        "\n",
        "**Why sorting?** Sorting is a fundamental operation in ranking, Top-K queries, and joins."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27b27e7c",
      "metadata": {
        "id": "27b27e7c"
      },
      "source": [
        "### üå∂Ô∏è SPICY CHALLENGE (Optional for Advanced Students)\n",
        "**Too easy?** Try this while others finish:\n",
        "- Sort by multiple columns: `['region', 'total_amount']`\n",
        "- Does the speedup increase or decrease? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc274c01",
      "metadata": {
        "id": "cc274c01"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üèãÔ∏è EXERCISE 2.2: BEAT THE CPU (SORTING)\n",
        "# =============================================================================\n",
        "# Your Goal: Sort the 10M row dataset by 'total_amount' on the GPU.\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üèÅ RACE START: SORTING 10 MILLION ROWS\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# 1. CPU BENCHMARK (Pandas)\n",
        "print(\"üî∑ CPU (Pandas): Sorting...\")\n",
        "start = time.time()\n",
        "df_sorted_cpu = df_pandas.sort_values('total_amount', ascending=False)\n",
        "cpu_time = time.time() - start\n",
        "print(f\"   ‚è±Ô∏è Time: {cpu_time:.3f} seconds\")\n",
        "del df_sorted_cpu\n",
        "\n",
        "# 2. GPU BENCHMARK (cuDF)\n",
        "print(\"\\nüî∂ GPU (cuDF): Sorting...\")\n",
        "\n",
        "if CUDF_AVAILABLE:\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Implement the GPU Sort!\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # 1. Convert the Pandas DataFrame to a cuDF DataFrame\n",
        "    print(\"   Moving data to GPU...\")\n",
        "    # Hint: cudf.DataFrame.from_pandas(...)\n",
        "\n",
        "    df_cudf = None # üî¥ YOUR CODE HERE\n",
        "\n",
        "    # 2. Sort by 'total_amount' descending\n",
        "    print(\"   Sorting on GPU...\")\n",
        "    start = time.time()\n",
        "\n",
        "    # Hint: It's the exact same syntax as Pandas!\n",
        "    df_sorted_gpu = None # üî¥ YOUR CODE HERE\n",
        "\n",
        "    # Wait for GPU to finish\n",
        "    cuda.synchronize()\n",
        "    gpu_time = time.time() - start\n",
        "    print(f\"   ‚è±Ô∏è Time: {gpu_time:.3f} seconds\")\n",
        "\n",
        "    # Visualize the win!\n",
        "    create_benchmark_bar_chart(cpu_time, gpu_time, \"Sorting 10M Rows\")\n",
        "\n",
        "    # Cleanup\n",
        "    del df_sorted_gpu\n",
        "else:\n",
        "    print(\"‚ùå cuDF not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30b62ce8",
      "metadata": {
        "id": "30b62ce8"
      },
      "source": [
        "### 2.3 Benchmark: GroupBy Aggregation (CPU vs. GPU)\n",
        "\n",
        "**Operation:** Group by `customer_id` and compute total spending per customer.\n",
        "\n",
        "**üõë STOP & PREDICT:**\n",
        "- GroupBy is harder to parallelize than Sort (requires coordination).\n",
        "- **My Prediction:** The speedup will be (Higher / Lower) than sorting.\n",
        "\n",
        "**Why GroupBy?** This is the bread-and-butter of customer segmentation (RFM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1ecdf3",
      "metadata": {
        "id": "4b1ecdf3"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üèãÔ∏è EXERCISE 2.3: BEAT THE CPU (GROUPBY)\n",
        "# =============================================================================\n",
        "# Your Goal: Group by 'customer_id' and calculate stats.\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üèÅ RACE START: GROUPBY AGGREGATION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# 1. CPU BENCHMARK\n",
        "print(\"üî∑ CPU (Pandas): Grouping...\")\n",
        "start = time.time()\n",
        "res_cpu = df_pandas.groupby('customer_id').agg({\n",
        "    'transaction_id': 'count',\n",
        "    'total_amount': ['sum', 'mean']\n",
        "})\n",
        "cpu_time = time.time() - start\n",
        "print(f\"   ‚è±Ô∏è Time: {cpu_time:.3f} seconds\")\n",
        "\n",
        "# 2. GPU BENCHMARK\n",
        "print(\"\\nüî∂ GPU (cuDF): Grouping...\")\n",
        "\n",
        "if CUDF_AVAILABLE:\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Implement the GPU GroupBy!\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Reuse 'df_cudf' from the previous exercise if it exists, else create it\n",
        "    if 'df_cudf' not in locals() or df_cudf is None:\n",
        "        df_cudf = cudf.DataFrame.from_pandas(df_pandas)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Perform the SAME aggregation on df_cudf\n",
        "    # Hint: .groupby('customer_id').agg({...})\n",
        "\n",
        "    res_gpu = None # üî¥ YOUR CODE HERE\n",
        "\n",
        "    cuda.synchronize()\n",
        "    gpu_time = time.time() - start\n",
        "    print(f\"   ‚è±Ô∏è Time: {gpu_time:.3f} seconds\")\n",
        "\n",
        "    create_benchmark_bar_chart(cpu_time, gpu_time, \"GroupBy Aggregation\")\n",
        "else:\n",
        "    print(\"‚ùå cuDF not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e21c24e",
      "metadata": {
        "id": "3e21c24e"
      },
      "source": [
        "### 2.4 Benchmark: Join/Merge Operations (CPU vs. GPU)\n",
        "\n",
        "**Operation:** Merge two DataFrames on a common key (simulating a database JOIN).\n",
        "\n",
        "**üõë STOP & PREDICT:**\n",
        "- Joins involve moving lots of data to match keys.\n",
        "- **My Prediction:** GPU memory bandwidth (will / will not) be the bottleneck.\n",
        "\n",
        "**Why Joins?** Fundamental for data enrichment and ETL pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89347204",
      "metadata": {
        "id": "89347204"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üèãÔ∏è EXERCISE 2.4: BEAT THE CPU (JOIN)\n",
        "# =============================================================================\n",
        "# Your Goal: Join transactions with a product table.\n",
        "# =============================================================================\n",
        "\n",
        "# Create a product table\n",
        "products = pd.DataFrame({\n",
        "    'product_id': np.arange(1, 5001, dtype=np.int32),\n",
        "    'category': ['Cat'+str(i) for i in range(5000)]\n",
        "})\n",
        "\n",
        "print(\"üèÅ RACE START: JOIN/MERGE\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# 1. CPU BENCHMARK\n",
        "print(\"üî∑ CPU (Pandas): Merging...\")\n",
        "start = time.time()\n",
        "merged_cpu = df_pandas.merge(products, on='product_id', how='left')\n",
        "cpu_time = time.time() - start\n",
        "print(f\"   ‚è±Ô∏è Time: {cpu_time:.3f} seconds\")\n",
        "\n",
        "# 2. GPU BENCHMARK\n",
        "print(\"\\nüî∂ GPU (cuDF): Merging...\")\n",
        "\n",
        "if CUDF_AVAILABLE:\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Implement the GPU Merge!\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 1. You need the product table on the GPU too!\n",
        "    products_gpu = None # üî¥ YOUR CODE HERE (convert 'products' to cuDF)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # 2. Perform the merge\n",
        "    # Hint: df_cudf.merge(..., on='product_id', how='left')\n",
        "\n",
        "    merged_gpu = None # üî¥ YOUR CODE HERE\n",
        "\n",
        "    cuda.synchronize()\n",
        "    gpu_time = time.time() - start\n",
        "    print(f\"   ‚è±Ô∏è Time: {gpu_time:.3f} seconds\")\n",
        "\n",
        "    create_benchmark_bar_chart(cpu_time, gpu_time, \"Join 10M Rows\")\n",
        "else:\n",
        "    print(\"‚ùå cuDF not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184a97cd",
      "metadata": {
        "id": "184a97cd"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 2.5: SUMMARY\n",
        "# =============================================================================\n",
        "# Let's look at the total time saved.\n",
        "# =============================================================================\n",
        "\n",
        "if 'cpu_time' in locals() and 'gpu_time' in locals():\n",
        "    benchmarks = [\n",
        "        {'name': 'Sort', 'cpu_time': cpu_time, 'gpu_time': gpu_time}, # Using last values\n",
        "        # In a real run we'd capture all, but this is a quick summary\n",
        "    ]\n",
        "    # Note: This summary chart is just a demo of the last operation\n",
        "    # to show the utility function working\n",
        "    create_combined_benchmark_summary(benchmarks)\n",
        "\n",
        "print(\"\\nüéâ PART 2 COMPLETE! You've proven that GPU > CPU for these tasks.\")\n",
        "print(\"Now let's look UNDER THE HOOD at how this works.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90a1d34b",
      "metadata": {
        "id": "90a1d34b"
      },
      "source": [
        "### 2.5 Key Takeaway: When GPUs Shine\n",
        "\n",
        "**Summary of Part 2:**\n",
        "\n",
        "| Operation | CPU Time | GPU Time | Speedup |\n",
        "|-----------|----------|----------|---------|\n",
        "| Sort 10M rows | ~X sec | ~Y sec | ~Nx |\n",
        "| GroupBy 100K groups | ~X sec | ~Y sec | ~Nx |\n",
        "| Join/Merge 10M √ó 5K | ~X sec | ~Y sec | ~Nx |\n",
        "\n",
        "**GPU acceleration works best when:**\n",
        "1. ‚úÖ Large data volumes (millions of rows)\n",
        "2. ‚úÖ Operations that parallelize well (element-wise, sort, aggregation, joins)\n",
        "3. ‚úÖ Repeated operations on the same dataset (data stays on GPU)\n",
        "\n",
        "**GPU acceleration works poorly when:**\n",
        "1. ‚ùå Small datasets (transfer overhead dominates)\n",
        "2. ‚ùå Complex sequential logic (Python loops)\n",
        "3. ‚ùå Frequent data transfers between CPU‚ÜîGPU\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ef1123",
      "metadata": {
        "id": "e4ef1123"
      },
      "source": [
        "## üìà Part 3: The \"How\" - GPU-Accelerated Data Mining Workflow\n",
        "\n",
        "### The Modern Approach: RAPIDS cuDF\n",
        "\n",
        "Now that you understand the fundamentals, let's see how production code is written.\n",
        "\n",
        "**The Reality:** You rarely write custom CUDA kernels for data mining!\n",
        "\n",
        "Instead, you use high-level libraries like RAPIDS cuDF that:\n",
        "1. ‚úÖ Provide Pandas-like API (minimal code changes)\n",
        "2. ‚úÖ Handle memory management automatically\n",
        "3. ‚úÖ Use highly optimized CUDA kernels internally\n",
        "4. ‚úÖ Support the full data mining workflow\n",
        "\n",
        "**The Stack:**\n",
        "```\n",
        "Your Code (Python)\n",
        "       ‚Üì\n",
        "  RAPIDS cuDF (Pandas-like API)\n",
        "       ‚Üì\n",
        "  libcudf (C++ CUDA Library)\n",
        "       ‚Üì\n",
        "  CUDA Runtime\n",
        "       ‚Üì\n",
        "  GPU Hardware\n",
        "```\n",
        "\n",
        "### 3.1 Data Mining Task: Customer Segmentation\n",
        "\n",
        "**Business Problem:** An e-commerce company wants to segment customers based on their purchasing behavior for targeted marketing.\n",
        "\n",
        "**RFM Analysis:**\n",
        "- **R**ecency: How recently did they purchase?\n",
        "- **F**requency: How often do they purchase?\n",
        "- **M**onetary: How much do they spend?\n",
        "\n",
        "We'll compute these metrics for 100,000 customers across 10 million transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1088ba05",
      "metadata": {
        "id": "1088ba05"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üèãÔ∏è EXERCISE 3.1: BUILD THE RFM PIPELINE\n",
        "# =============================================================================\n",
        "# Task: Perform a full RFM analysis on the GPU.\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üèóÔ∏è BUILDING RFM PIPELINE\")\n",
        "\n",
        "if CUDF_AVAILABLE:\n",
        "    # STEP 1: MOVE DATA TO GPU (ONCE!)\n",
        "    print(\"1Ô∏è‚É£ Moving data to GPU...\")\n",
        "    start = time.time()\n",
        "    gdf = cudf.DataFrame.from_pandas(df_pandas)\n",
        "    print(f\"   Done in {time.time()-start:.2f}s\")\n",
        "\n",
        "    # STEP 2: CALCULATE R, F, M VALUES\n",
        "    print(\"\\n2Ô∏è‚É£ Calculating R, F, M values...\")\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: GroupBy 'customer_id' and calculate:\n",
        "    # - max transaction_date (Recency)\n",
        "    # - count transaction_id (Frequency)\n",
        "    # - sum total_amount (Monetary)\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Hint: Use .agg({'transaction_date': 'max', 'transaction_id': 'count', 'total_amount': 'sum'})\n",
        "\n",
        "    rfm = None # üî¥ YOUR CODE HERE\n",
        "\n",
        "    # Rename columns for clarity\n",
        "    if rfm is not None:\n",
        "        # ‚ö†Ô∏è CAUTION: Ensure your aggregation order matches the names below!\n",
        "        # If you used the hint above, this order is correct.\n",
        "        rfm.columns = ['last_date', 'frequency', 'monetary']\n",
        "\n",
        "        # Calculate days since last purchase\n",
        "        ref_date = pd.Timestamp('2024-11-27')\n",
        "        rfm['recency'] = (ref_date - rfm['last_date']).dt.days\n",
        "\n",
        "        print(f\"   Calculated RFM for {len(rfm)} customers\")\n",
        "        print(rfm.head())\n",
        "\n",
        "        # STEP 3: VISUALIZE\n",
        "        print(\"\\n3Ô∏è‚É£ Visualizing Segments...\")\n",
        "        # Simple scoring for viz\n",
        "        rfm['RFM_score'] = (rfm['frequency'] > 5).astype(int) + (rfm['monetary'] > 1000).astype(int) * 10\n",
        "        visualize_rfm_segments(rfm)\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå cuDF not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21999e1d",
      "metadata": {
        "id": "21999e1d"
      },
      "source": [
        "### 3.2 Code Comparison: Pandas vs. cuDF\n",
        "\n",
        "One of RAPIDS' greatest strengths is **API compatibility**. Let's see them side-by-side:\n",
        "\n",
        "| Operation | Pandas (CPU) | cuDF (GPU) |\n",
        "|-----------|-------------|------------|\n",
        "| Import | `import pandas as pd` | `import cudf` |\n",
        "| Read CSV | `pd.read_csv('file.csv')` | `cudf.read_csv('file.csv')` |\n",
        "| From DataFrame | - | `cudf.DataFrame.from_pandas(df)` |\n",
        "| GroupBy | `df.groupby('col').agg({...})` | `gdf.groupby('col').agg({...})` |\n",
        "| Sort | `df.sort_values('col')` | `gdf.sort_values('col')` |\n",
        "| Filter | `df[df['col'] > 5]` | `gdf[gdf['col'] > 5]` |\n",
        "| To Pandas | - | `gdf.to_pandas()` |\n",
        "\n",
        "**The takeaway:** If you know Pandas, you already know 90% of cuDF!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f8fbf8d",
      "metadata": {
        "id": "2f8fbf8d"
      },
      "source": [
        "### 3.3 Best Practices for GPU-Accelerated Data Mining\n",
        "\n",
        "**The Golden Rules:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aabc30d0",
      "metadata": {
        "id": "aabc30d0"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 3.3: BEST PRACTICES DEMONSTRATION\n",
        "# =============================================================================\n",
        "# This cell demonstrates common mistakes and best practices.\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"BEST PRACTICES FOR GPU-ACCELERATED DATA MINING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# RULE 1: Keep data on GPU - Avoid unnecessary transfers\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüìå RULE 1: KEEP DATA ON GPU\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if CUDF_AVAILABLE:\n",
        "    try:\n",
        "        # BAD: Transfer back and forth for each operation\n",
        "        print(\"\\n‚ùå BAD PATTERN (frequent transfers):\")\n",
        "\n",
        "        gc.collect()\n",
        "        start_bad = time.time()\n",
        "\n",
        "        # Simulating bad pattern: transfer for each operation\n",
        "        df_gpu = cudf.DataFrame.from_pandas(df_pandas[['customer_id', 'total_amount']].head(1_000_000))\n",
        "        temp_pandas = df_gpu.to_pandas()  # Unnecessary transfer!\n",
        "        temp_pandas['doubled'] = temp_pandas['total_amount'] * 2\n",
        "        df_gpu = cudf.DataFrame.from_pandas(temp_pandas)  # Transfer back!\n",
        "        temp_pandas = df_gpu.to_pandas()  # Transfer again!\n",
        "        temp_pandas['tripled'] = temp_pandas['total_amount'] * 3\n",
        "\n",
        "        bad_time = time.time() - start_bad\n",
        "        print(f\"   Time with unnecessary transfers: {bad_time:.3f} seconds\")\n",
        "\n",
        "        del df_gpu, temp_pandas\n",
        "        gc.collect()\n",
        "\n",
        "        # GOOD: Keep all operations on GPU\n",
        "        print(\"\\n‚úÖ GOOD PATTERN (stay on GPU):\")\n",
        "\n",
        "        start_good = time.time()\n",
        "\n",
        "        df_gpu = cudf.DataFrame.from_pandas(df_pandas[['customer_id', 'total_amount']].head(1_000_000))\n",
        "        df_gpu['doubled'] = df_gpu['total_amount'] * 2  # Stays on GPU\n",
        "        df_gpu['tripled'] = df_gpu['total_amount'] * 3  # Stays on GPU\n",
        "        # Only transfer at the very end when you need results\n",
        "        final_result = df_gpu.to_pandas()\n",
        "\n",
        "        good_time = time.time() - start_good\n",
        "        print(f\"   Time with GPU-only operations: {good_time:.3f} seconds\")\n",
        "        print(f\"   Improvement: {bad_time/good_time:.1f}x faster!\")\n",
        "\n",
        "        del df_gpu, final_result\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   ‚ùå GPU demonstration failed: {type(e).__name__}\")\n",
        "        print(\"   GPU memory may be in an inconsistent state.\")\n",
        "        print(\"   Try: Runtime ‚Üí Restart runtime\")\n",
        "        print(\"\")\n",
        "        print(\"   üìñ EXPECTED RESULTS:\")\n",
        "        print(\"   ‚ùå BAD PATTERN:  ~0.5-1.0 seconds\")\n",
        "        print(\"   ‚úÖ GOOD PATTERN: ~0.1-0.3 seconds\")\n",
        "        print(\"   Improvement: ~2-5x faster!\")\n",
        "else:\n",
        "    print(\"\\n   (cuDF not available - skipping GPU demonstration)\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# RULE 2: Use appropriate data types\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüìå RULE 2: USE APPROPRIATE DATA TYPES\")\n",
        "print(\"-\" * 50)\n",
        "print(\"\"\"\n",
        "   ‚úÖ Use int32 instead of int64 when possible (half the memory!)\n",
        "   ‚úÖ Use float32 instead of float64 for most ML tasks\n",
        "   ‚úÖ Use categorical types for string columns with few unique values\n",
        "\n",
        "   Memory impact example:\n",
        "   - 10M rows √ó int64: 80 MB\n",
        "   - 10M rows √ó int32: 40 MB (50% reduction!)\n",
        "\n",
        "   GPU memory is precious - optimize your dtypes!\n",
        "\"\"\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# RULE 3: Batch operations\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüìå RULE 3: BATCH YOUR OPERATIONS\")\n",
        "print(\"-\" * 50)\n",
        "print(\"\"\"\n",
        "   ‚ùå BAD:  Running 1000 small queries one at a time\n",
        "   ‚úÖ GOOD: Combining into larger batch operations\n",
        "\n",
        "   Each kernel launch has overhead. Minimize the number of operations!\n",
        "\"\"\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# RULE 4: Profile before optimizing\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüìå RULE 4: PROFILE BEFORE OPTIMIZING\")\n",
        "print(\"-\" * 50)\n",
        "print(\"\"\"\n",
        "   Always measure before assuming GPU will be faster!\n",
        "\n",
        "   Tools:\n",
        "   - Python's time module (what we've been using)\n",
        "   - cuDF's built-in profiler\n",
        "   - NVIDIA Nsight Systems for detailed GPU profiling\n",
        "\n",
        "   Questions to ask:\n",
        "   - Is the dataset large enough? (< 100K rows: CPU might win)\n",
        "   - What's the transfer time vs compute time ratio?\n",
        "   - Are there operations cuDF doesn't accelerate well?\n",
        "\"\"\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# RULE 5: Handle memory carefully\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüìå RULE 5: MANAGE GPU MEMORY\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if NUMBA_AVAILABLE:\n",
        "    try:\n",
        "        free_mem, total_mem = cuda.current_context().get_memory_info()\n",
        "        print(f\"   Current GPU memory: {free_mem/1e9:.2f} GB free / {total_mem/1e9:.2f} GB total\")\n",
        "    except Exception as e:\n",
        "        print(f\"   (GPU memory query failed - may be in inconsistent state)\")\n",
        "    print(\"\"\"\n",
        "   Tips:\n",
        "   - Delete DataFrames when no longer needed: del df_gpu\n",
        "   - Call gc.collect() after deletions\n",
        "   - Monitor memory with nvidia-smi\n",
        "   - For very large datasets, process in chunks\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "047d713e",
      "metadata": {
        "id": "047d713e"
      },
      "source": [
        "---\n",
        "\n",
        "## üíÄ Part 4: The Final Boss - The Impossible Travel Detector\n",
        "\n",
        "### The Scenario\n",
        "You are a Data Engineer at a global bank. You process millions of login events every hour.\n",
        "Your security team has identified a new fraud pattern: **\"Impossible Travelers\"**.\n",
        "\n",
        "These are hackers who:\n",
        "1.  Log in to an account from **London** at 1:00 PM.\n",
        "2.  Log in to the *same* account from **New York** at 2:00 PM.\n",
        "\n",
        "**The Problem:** The distance between London and New York is ~5,500 km. No plane flies that fast (Mach 4+). This is physically impossible and indicates a shared credential attack.\n",
        "\n",
        "### The Challenge\n",
        "You have a dataset of 1 million logins with `latitude`, `longitude`, and `timestamp`.\n",
        "You must identify all users who moved faster than **800 km/h** (approximate plane speed).\n",
        "\n",
        "**Why this is hard:**\n",
        "- You need to compare row $i$ with row $i-1$ (sequential dependency).\n",
        "- You need to calculate the **Haversine Distance** (complex trigonometry) for every pair.\n",
        "- SQL and Pandas are slow at \"row vs previous row\" logic on millions of groups.\n",
        "- **Solution:** A custom CUDA kernel!\n",
        "\n",
        "### 4.1 Generate Data\n",
        "First, let's generate a dataset with some injected fraud cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c532636",
      "metadata": {
        "id": "5c532636"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 4.1: GENERATE TRAVEL DATA\n",
        "# =============================================================================\n",
        "# We need a dataset with locations (lat/lon) and timestamps.\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üåç Generating Travel Data...\")\n",
        "\n",
        "# 1. Create a focused dataset for this challenge\n",
        "# We'll use 1 million logins to keep it manageable but significant\n",
        "n_logins = 1_000_000\n",
        "n_users = 50_000\n",
        "\n",
        "# Generate base data\n",
        "logins = pd.DataFrame({\n",
        "    'user_id': np.random.randint(1, n_users + 1, size=n_logins, dtype=np.int32),\n",
        "    'timestamp': pd.Timestamp('2024-01-01') + pd.to_timedelta(np.random.randint(0, 60*24*30, size=n_logins), unit='min')\n",
        "})\n",
        "\n",
        "# Generate locations (approximate UK/US coordinates for realism)\n",
        "# Most users are in UK (50-55 lat, -5 to 2 lon)\n",
        "lat_uk = np.random.uniform(50, 55, size=n_logins)\n",
        "lon_uk = np.random.uniform(-5, 2, size=n_logins)\n",
        "\n",
        "# Some users are in US (30-45 lat, -120 to -70 lon)\n",
        "lat_us = np.random.uniform(30, 45, size=n_logins)\n",
        "lon_us = np.random.uniform(-120, -70, size=n_logins)\n",
        "\n",
        "# Assign 90% to UK, 10% to US\n",
        "mask = np.random.random(n_logins) > 0.9\n",
        "logins['latitude'] = np.where(mask, lat_us, lat_uk).astype(np.float32)\n",
        "logins['longitude'] = np.where(mask, lon_us, lon_uk).astype(np.float32)\n",
        "\n",
        "# INJECT FRAUD: Create \"Impossible Travelers\"\n",
        "# Pick 100 random users and force a UK -> US jump in 1 hour\n",
        "# We append specific fraud pairs to ensure they exist\n",
        "fraud_cases = pd.DataFrame({\n",
        "    'user_id': np.random.randint(90000, 99999, size=50), # Special IDs\n",
        "    'timestamp': [pd.Timestamp('2024-01-01 12:00:00')] * 50,\n",
        "    'latitude': [51.5074] * 50, # London\n",
        "    'longitude': [-0.1278] * 50\n",
        "})\n",
        "fraud_cases_jump = fraud_cases.copy()\n",
        "fraud_cases_jump['timestamp'] = fraud_cases['timestamp'] + pd.Timedelta(hours=2)\n",
        "fraud_cases_jump['latitude'] = 40.7128 # NYC\n",
        "fraud_cases_jump['longitude'] = -74.0060\n",
        "\n",
        "# Combine and shuffle\n",
        "logins = pd.concat([logins, fraud_cases, fraud_cases_jump]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(f\"‚úÖ Generated {len(logins)} login events.\")\n",
        "print(logins.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51bcc65b",
      "metadata": {
        "id": "51bcc65b"
      },
      "source": [
        "### 4.2 The Challenge (Numba Kernel)\n",
        "\n",
        "**The Logic:**\n",
        "1.  **Sort** the data by `user_id` and `timestamp`. This places consecutive events for the same user next to each other in memory.\n",
        "2.  **Launch** one thread per row.\n",
        "3.  **Check**: If `user_id[i] == user_id[i-1]`, calculate the distance and time difference.\n",
        "4.  **Flag**: If speed > 800 km/h, mark as suspicious.\n",
        "\n",
        "**The Math (Haversine Formula):**\n",
        "$$ a = \\sin^2(\\frac{\\Delta\\phi}{2}) + \\cos \\phi_1 \\cdot \\cos \\phi_2 \\cdot \\sin^2(\\frac{\\Delta\\lambda}{2}) $$\n",
        "$$ c = 2 \\cdot \\text{atan2}(\\sqrt{a}, \\sqrt{1-a}) $$\n",
        "$$ d = R \\cdot c $$\n",
        "Where $\\phi$ is latitude, $\\lambda$ is longitude, $R$ is earth radius (6371 km).\n",
        "All angles must be in **radians**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5784129",
      "metadata": {
        "id": "e5784129"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üèãÔ∏è EXERCISE 4.2: THE IMPOSSIBLE TRAVEL DETECTOR\n",
        "# =============================================================================\n",
        "# YOUR MISSION: Write a kernel to detect impossible travel speeds.\n",
        "# =============================================================================\n",
        "\n",
        "if CUDF_AVAILABLE and NUMBA_AVAILABLE:\n",
        "    from math import sin, cos, sqrt, atan2, radians\n",
        "\n",
        "    # 1. PREPARE DATA\n",
        "    print(\"1Ô∏è‚É£ Moving and Sorting Data...\")\n",
        "    gdf_logins = cudf.DataFrame.from_pandas(logins)\n",
        "\n",
        "    # TODO: Sort by 'user_id' and 'timestamp'\n",
        "    # gdf_logins = ...\n",
        "\n",
        "    # Convert timestamp to seconds for the kernel\n",
        "    gdf_logins['time_sec'] = gdf_logins['timestamp'].astype('int64') // 10**9\n",
        "\n",
        "    # 2. DEFINE THE KERNEL\n",
        "    @cuda.jit\n",
        "    def impossible_travel_kernel(lat, lon, time, user_id, suspicious_flags):\n",
        "        i = cuda.grid(1)\n",
        "\n",
        "        if i > 0 and i < lat.size:\n",
        "            # -----------------------------------------------------------------\n",
        "            # TODO 1: Check if current row 'i' is same user as 'i-1'\n",
        "            # -----------------------------------------------------------------\n",
        "\n",
        "            # -----------------------------------------------------------------\n",
        "            # TODO 2: Calculate Time Difference (in hours)\n",
        "            # -----------------------------------------------------------------\n",
        "\n",
        "            # -----------------------------------------------------------------\n",
        "            # TODO 3: Calculate Haversine Distance (km)\n",
        "            # Formula:\n",
        "            # a = sin¬≤(ŒîœÜ/2) + cos œÜ1 ‚ãÖ cos œÜ2 ‚ãÖ sin¬≤(ŒîŒª/2)\n",
        "            # c = 2 ‚ãÖ atan2( ‚àöa, ‚àö(1‚àía) )\n",
        "            # d = R ‚ãÖ c\n",
        "            # (Use radians, sin, etc. from the imports above)\n",
        "            # -----------------------------------------------------------------\n",
        "\n",
        "            # -----------------------------------------------------------------\n",
        "            # TODO 4: Calculate Speed & Flag if > 800 km/h\n",
        "            # -----------------------------------------------------------------\n",
        "            pass\n",
        "\n",
        "    # 3. LAUNCH\n",
        "    print(\"2Ô∏è‚É£ Launching Kernel...\")\n",
        "    suspicious = cudf.Series(np.zeros(len(gdf_logins), dtype=np.int32))\n",
        "\n",
        "    # TODO: Configure Grid\n",
        "    # threads = ...\n",
        "    # blocks = ...\n",
        "\n",
        "    # TODO: Launch Kernel\n",
        "    # impossible_travel_kernel[...](...)\n",
        "\n",
        "    # 4. REPORT\n",
        "    gdf_logins['suspicious'] = suspicious\n",
        "    print(f\"\\nüö® Found {gdf_logins['suspicious'].sum()} suspicious events.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå GPU libraries not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b2e2a75",
      "metadata": {
        "id": "7b2e2a75"
      },
      "source": [
        "---\n",
        "\n",
        "## üßπ Cleanup & Memory Management\n",
        "\n",
        "Always clean up GPU resources when done to prevent memory leaks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22190549",
      "metadata": {
        "id": "22190549"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CLEANUP CELL\n",
        "# =============================================================================\n",
        "# Run this cell to free GPU memory when you're done experimenting.\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üßπ Cleaning up GPU memory...\")\n",
        "\n",
        "# Delete GPU DataFrames if they exist\n",
        "# Using globals() is more reliable than dir() for this purpose\n",
        "gpu_vars = [\n",
        "    'df_cudf', 'df_sorted_gpu', 'res_gpu', 'products_gpu', 'merged_gpu',\n",
        "    'gdf', 'rfm', 'gdf_demo', 'd_data', 'a', 'b', 'c', 'A', 'B', 'C', 'img_gpu'\n",
        "]\n",
        "\n",
        "for var in gpu_vars:\n",
        "    if var in globals():\n",
        "        del globals()[var]\n",
        "        print(f\"   Deleted {var}\")\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Clear CUDA memory cache\n",
        "if NUMBA_AVAILABLE:\n",
        "    try:\n",
        "        cuda.current_context().memory_manager.deallocations.clear()\n",
        "    except:\n",
        "        pass  # May fail if no allocations to clear\n",
        "\n",
        "# Report final memory state\n",
        "if NUMBA_AVAILABLE:\n",
        "    try:\n",
        "        free_mem, total_mem = cuda.current_context().get_memory_info()\n",
        "        print(f\"\\n   GPU Memory: {free_mem/1e9:.2f} GB free / {total_mem/1e9:.2f} GB total\")\n",
        "    except (NotImplementedError, Exception):\n",
        "        # RMM allocator doesn't support get_memory_info()\n",
        "        print(\"\\n   (GPU memory info not available with RMM allocator)\")\n",
        "\n",
        "print(\"\\n‚úÖ Cleanup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82419614",
      "metadata": {
        "id": "82419614"
      },
      "source": [
        "---\n",
        "\n",
        "## üìö Workshop Summary\n",
        "\n",
        "### üèãÔ∏è Exercises Completed\n",
        "\n",
        "| Exercise | Topic | Skills Practiced |\n",
        "|----------|-------|------------------|\n",
        "| **Exercise 1** | RAPIDS (cuDF) | Sorting, GroupBy, Joins on GPU |\n",
        "| **Exercise 2.3** | Vector Addition Kernel | Thread indexing, bounds checking, 1D grids |\n",
        "| **Exercise 2.5** | Matrix Multiplication | 2D grids, dot product, 2D bounds |\n",
        "| **Exercise 3.1** | RFM Pipeline | End-to-end GPU data mining workflow |\n",
        "| **Exercise 4.2** | Impossible Travel | Custom Kernels, Haversine, Sequential Logic |\n",
        "\n",
        "### What We Covered\n",
        "\n",
        "| Part | Topic | Key Takeaway |\n",
        "|------|-------|--------------|\n",
        "| **Part 1** | Benchmarking | GPU can be 10-50x faster for large datasets |\n",
        "| **Part 2** | CUDA Fundamentals | Grid ‚Üí Block ‚Üí Thread hierarchy; memory management is critical |\n",
        "| **Part 3** | Data Mining Workflow | cuDF provides Pandas-like API with GPU acceleration |\n",
        "| **Part 4** | Advanced Kernels | Solving complex logic (row-wise dependencies) with Numba |\n",
        "\n",
        "### The GPU Computing Stack\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  YOUR CODE                                                          ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ When to use: Always start here!                               ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  RAPIDS (cuDF, cuML, cuGraph)                                       ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ When to use: 90% of data mining tasks                         ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ Pandas/Scikit-learn-like APIs                                 ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  Numba CUDA                                                         ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ When to use: Custom operations not in RAPIDS                  ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ Write Python, runs on GPU                                     ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  Raw CUDA (C/C++)                                                   ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ When to use: Maximum performance, library development         ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ Rarely needed for data mining                                 ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### Common Student Mistakes Checklist\n",
        "\n",
        "- [ ] ‚ùå Forgot to enable GPU runtime (Runtime ‚Üí Change runtime type ‚Üí T4 GPU)\n",
        "- [ ] ‚ùå Didn't restart runtime after installing cuDF\n",
        "- [ ] ‚ùå Transferring data between CPU/GPU too frequently\n",
        "- [ ] ‚ùå Using int64/float64 when int32/float32 would suffice\n",
        "- [ ] ‚ùå Not including bounds checking in custom kernels\n",
        "- [ ] ‚ùå Expecting GPU speedup on small datasets (< 100K rows)\n",
        "- [ ] ‚ùå Forgetting cuda.synchronize() when timing GPU operations\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Practice:** Modify this notebook with your own datasets\n",
        "2. **Explore:** Check out cuML for GPU-accelerated machine learning\n",
        "3. **Read:** RAPIDS documentation at https://docs.rapids.ai/\n",
        "4. **Experiment:** Try different block sizes and measure performance\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Additional Exercises (Take-Home)\n",
        "\n",
        "1. **Modify the data size:** Change `N_ROWS` to 5M and 20M. How does speedup change?\n",
        "2. **New benchmark:** Add a filtering benchmark (e.g., `df[df['total_amount'] > 100]`)\n",
        "3. **Advanced kernel:** Write a kernel that computes `sqrt(a^2 + b^2)` for two vectors\n",
        "4. **Real data:** Download a large CSV from Kaggle and compare loading times\n",
        "\n",
        "---\n",
        "\n",
        "**üìå STUDENT VERSION** - Compare with instructor notebook for complete solutions!\n",
        "\n",
        "*Questions? Raise your hand or post in the course forum!*\n",
        "*Questions? Raise your hand or post in the course forum!**Questions? Raise your hand or post in the course forum!*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}